{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Activation, BatchNormalization, Rescaling\n",
    "from keras.layers import RandomContrast, RandomZoom, RandomFlip, RandomRotation, RandomTranslation, RandomCrop, RandomBrightness\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/train/_classes.csv\")\n",
    "dict_disease = {}\n",
    "\n",
    "for number, name in enumerate(df.columns):\n",
    "    if name == 'filename':continue              # skip 1st column\n",
    "    dict_disease[number-1] = name.strip()\n",
    "dict_disease = {key: dict_disease[key] for key in dict_disease if key != 'Unlabeled'}\n",
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImg(filenames, labels, size, resize, rescale):\n",
    "    images = []\n",
    "    for i, name in enumerate(nameMsg := tqdm(filenames)):\n",
    "        nameMsg.set_postfix_str(name)\n",
    "        try:\n",
    "            img = tf.keras.preprocessing.image.load_img(name)\n",
    "        except:\n",
    "            labels = np.delete(labels, i)\n",
    "            continue\n",
    "        img = tf.cast(img, tf.float32)\n",
    "\n",
    "        if resize:\n",
    "            img = tf.image.resize(img, (size, size))\n",
    "\n",
    "        if rescale == 1:\n",
    "            img = img / 255\n",
    "        elif rescale == 2:\n",
    "            img = (img / 127.5) - 1\n",
    "        elif rescale == 0:\n",
    "            img = tf.cast(img, tf.uint8)\n",
    "\n",
    "        images.append(img)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def parseData(filename, delimiter=',', size=False, resize=False, rescale=1, return_format='dataset'):\n",
    "    if resize:\n",
    "        if not size:\n",
    "            raise Exception('Size must be specified when resize is true.')\n",
    "\n",
    "    if return_format.lower() not in ['dataset', 'split']:\n",
    "        raise Exception(\"Return format unspecified.\")\n",
    "    \n",
    "    loaded_dataset = pd.read_csv(str(filename)+\"_classes.csv\", delimiter=delimiter)\n",
    "    img_dir = np.array(loaded_dataset.pop('filename'))\n",
    "    img_dir = np.ndarray.flatten(img_dir)\n",
    "\n",
    "    for row, img_path in enumerate(img_dir):\n",
    "        img_dir[row] = os.path.join(filename, img_path)\n",
    "\n",
    "    labels = np.array(loaded_dataset.idxmax(axis=1).str.strip().astype(\n",
    "        'category').cat.codes).reshape(-1, 1)\n",
    "\n",
    "    images, labels = loadImg(img_dir, labels, size, resize, rescale)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if return_format == 'dataset':\n",
    "        return dataset\n",
    "    elif return_format == 'split':\n",
    "        return images, labels\n",
    "    else:\n",
    "        raise Exception(\"Return format unspecified.\")\n",
    "\n",
    "\n",
    "def confusionMatrix(epoch, logs):\n",
    "    yhat = model.predict(test_images)\n",
    "    yhat = np.argmax(yhat, axis=1)\n",
    "    cm = confusion_matrix(test_labels, yhat)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.matshow(cm)\n",
    "    for (x, y), value in np.ndenumerate(cm):\n",
    "        plt.text(x, y, f\"{value:.2f}\", va=\"center\", ha=\"center\", color='white')\n",
    "    ax.set_title(f\"Confusion Matrix on epoch {epoch}\\nVal accuracy: {logs.get('val_accuracy'):.2f}\")\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    fig.savefig(f\"./CM/CM Epoch {epoch}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    train_dataset = parseData(\"./dataset/train/\", size=img_size,\n",
    "                              resize=True, rescale=False) # Rescale = 1 untuk normalisasi [0,1], 2 untuk normalisasi [-1,1], 0 kalau gambar untuk print, selain itu gambar tdk diproses\n",
    "    valid_dataset = parseData(\"./dataset/valid/\", size=img_size,\n",
    "                              resize=True, rescale=False) # Rescale = 1 untuk normalisasi [0,1], 2 untuk normalisasi [-1,1], 0 kalau gambar untuk print, selain itu gambar tdk diproses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    test_images_data, test_labels = parseData(\"./dataset/test/\", size=img_size,\n",
    "                                 resize=True, rescale=False, return_format='split') # Rescale = 1 untuk normalisasi [0,1], 2 untuk normalisasi [-1,1], 0 kalau gambar untuk print, selain itu gambar tdk diproses\n",
    "    \n",
    "    test_images = tf.data.Dataset.from_tensor_slices(test_images_data)\n",
    "    test_images = test_images.cache().batch(32).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_dataset.cache('train_dataset').shuffle(128).batch(32).prefetch(AUTOTUNE)\n",
    "valid = valid_dataset.cache().batch(32).prefetch(AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buat Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input((img_size, img_size, 3)),\n",
    "    Rescaling(1/255),               # range [0,1]\n",
    "    # Rescaling(1/127.5, offset=-1),  # range [-1,1]\n",
    "    RandomFlip(),\n",
    "    RandomRotation(factor=0.3),\n",
    "    RandomZoom(height_factor=(-0.1, 0.1)),\n",
    "    RandomBrightness(factor=(-0.1,0.1), value_range=[0,1]),\n",
    "    base_model,\n",
    "    # Dense(512, 'selu'),\n",
    "    # Dropout(0.2),\n",
    "    # Dense(32, 'selu'),\n",
    "    Dense(len(dict_disease.keys()), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    optimizer=Adam(0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "bestCB = ModelCheckpoint(filepath='./checkpoint/best/', monitor='val_accuracy',\n",
    "                         mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "cmCB = LambdaCallback(on_epoch_end=confusionMatrix)\n",
    "\n",
    "model.build([None, img_size, img_size, 3])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "# with tf.device('/gpu:0'):\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=valid,\n",
    "        epochs=100,\n",
    "        callbacks=[bestCB, cmCB]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./checkpoint/latest/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = history.epoch\n",
    "\n",
    "plt.plot(epochs, acc, label='Train Accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model's Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, label='Train Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Model's Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluasi Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iterasi terakhir\n",
    "# model = tf.keras.models.load_model(\"./checkpoint/latest/\")\n",
    "\n",
    "# Load terbaik\n",
    "model = tf.keras.models.load_model(\"./checkpoint/best/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images_data, test_labels)).cache().batch(32).prefetch(AUTOTUNE)\n",
    "eval_res = model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"Test Dataset\\nAccuracy: {eval_res[1]*100:.3f}%\\nLoss: {eval_res[0]:.3f}\")\n",
    "\n",
    "topK = 3 # Ambil 3 kategori tertinggi untuk display ke user\n",
    "for image, label in test_dataset.unbatch().shuffle(128).take(5):\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "    image           = tf.cast(image, tf.float32) / 255\n",
    "    image           = tf.expand_dims(image, axis=0)\n",
    "    yhat            = model.predict(image)\n",
    "    yhat_topk       = np.argpartition(-yhat, topK-1)[0][:3]\n",
    "    true_label      = dict_disease[label.numpy()[0]]\n",
    "    prob_res        = []\n",
    "    disease_types   = []\n",
    "\n",
    "    for disease in yhat_topk:\n",
    "        disease_types.append(dict_disease[disease])\n",
    "        prob_res.append(yhat[0][disease])\n",
    "    \n",
    "    print(f\"Label asli: {true_label}.\")\n",
    "    for index_disease, disease in enumerate(disease_types):\n",
    "        res = \"benar\" if true_label == disease else \"salah\"\n",
    "        print(f\"Prediksi {index_disease+1} adalah: {disease} ({res}) dengan probabilitas {prob_res[index_disease] * 100:.2f}%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
