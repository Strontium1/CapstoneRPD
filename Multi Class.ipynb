{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Activation, BatchNormalization, Rescaling\n",
    "from keras.layers import RandomContrast, RandomZoom, RandomFlip, RandomRotation, RandomTranslation, RandomCrop, RandomBrightness\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/train/_classes.csv\")\n",
    "dict_disease = {}\n",
    "\n",
    "for number, name in enumerate(df.columns):\n",
    "    if name == 'filename':continue              # skip 1st column\n",
    "    dict_disease[number-1] = name.strip()\n",
    "dict_disease = {key: dict_disease[key] for key in dict_disease if key != 'Unlabeled'}\n",
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(path, label, image_size=(224,224), return_label=False):\n",
    "    labels = pd.read_csv(label)\n",
    "    labels = labels.sort_values('filename')\n",
    "    labels = labels.loc[:, labels.columns != 'filename']\n",
    "    labels = np.array(labels.idxmax(axis=1).str.strip().astype(\n",
    "            'category').cat.codes).reshape(-1, 1)\n",
    "    \n",
    "    if return_label:\n",
    "        return labels\n",
    "    images = tf.keras.utils.image_dataset_from_directory(path, shuffle=False, batch_size=None, label_mode=None, image_size=image_size)\n",
    "    labels = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((images, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    path_train = \"./dataset/train/\"\n",
    "    path_valid = \"./dataset/valid/\"\n",
    "    path_test = \"./dataset/test/\"\n",
    "    train_dataset = loadImage(path_train, path_train+\"_classes.csv\", image_size=(img_size,img_size))\n",
    "    valid_dataset = loadImage(path_valid, path_valid+\"_classes.csv\", image_size=(img_size,img_size))\n",
    "    test_dataset = loadImage(path_test, path_test+\"_classes.csv\", image_size=(img_size,img_size))\n",
    "    test_labels = loadImage(path_test, path_test+\"_classes.csv\", return_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    train = train_dataset.repeat(1).cache('train_dataset').shuffle(256, reshuffle_each_iteration=True).map(\n",
    "        lambda image, label: (tf.cast(image, tf.float32), label)).batch(32).prefetch(AUTOTUNE)\n",
    "    valid = valid_dataset.cache('valid-dataset').batch(32).prefetch(AUTOTUNE)\n",
    "    test = test_dataset.cache('test-dataset').batch(32).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(epoch, logs):\n",
    "    yhat = model.predict(test)\n",
    "    yhat = np.argmax(yhat, axis=1)\n",
    "    cm = confusion_matrix(test_labels, yhat)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.matshow(cm)\n",
    "    for (x, y), value in np.ndenumerate(cm):\n",
    "        plt.text(x, y, f\"{value:.2f}\", va=\"center\", ha=\"center\", color='white')\n",
    "    ax.set_title(f\"Confusion Matrix on epoch {epoch}\\nVal accuracy: {logs.get('val_accuracy'):.2f}\")\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    try:\n",
    "        os.makedirs(\"./CM/\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    fig.savefig(f\"./CM/Epoch {epoch+1}.jpg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buat Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV3Large(input_shape=(img_size,img_size,3), include_top=False, include_preprocessing=False, pooling='max')\n",
    "base_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input((img_size, img_size, 3)),\n",
    "    Rescaling(1/255),               # range [0,1]\n",
    "    # Rescaling(1/127.5, offset=-1),  # range [-1,1]\n",
    "    RandomFlip(),\n",
    "    RandomRotation(factor=0.3),\n",
    "    RandomZoom(height_factor=(-0.2, 0.2)),\n",
    "    RandomBrightness(factor=(-0.2,0.2), value_range=[0,1]),\n",
    "    base_model,\n",
    "    Dense(512, 'selu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, 'selu'),\n",
    "    Dense(len(dict_disease.keys()), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    optimizer=Adam(0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "bestCB = ModelCheckpoint(filepath='./checkpoint/best/', monitor='val_accuracy',\n",
    "                         mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "cmCB = LambdaCallback(on_epoch_end=confusionMatrix)\n",
    "\n",
    "esCB = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
    "\n",
    "model.build([None, img_size, img_size, 3])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/cpu:0'):\n",
    "with tf.device('/gpu:0'):\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=valid,\n",
    "        epochs=10,\n",
    "        callbacks=[bestCB, cmCB, esCB]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./checkpoint/latest/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = history.epoch\n",
    "\n",
    "plt.plot(epochs, acc, label='Train Accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model's Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, label='Train Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Model's Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluasi Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iterasi terakhir\n",
    "# model = tf.keras.models.load_model(\"./checkpoint/latest/\")\n",
    "\n",
    "# Load terbaik\n",
    "model = tf.keras.models.load_model(\"./checkpoint/best/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test, verbose=0)\n",
    "print(f\"Loss: {test_loss:.4f}\\nAccuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = 3 # Ambil 3 kategori tertinggi untuk display ke user\n",
    "for image, label in test_dataset.shuffle(128).take(5):\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "    image           = tf.cast(image, tf.float32) / 255\n",
    "    image           = tf.expand_dims(image, axis=0)\n",
    "    yhat            = model.predict(image)\n",
    "    yhat_topk       = np.argpartition(-yhat, topK-1)[0][:3]\n",
    "    true_label      = dict_disease[label.numpy()[0]]\n",
    "    prob_res        = []\n",
    "    disease_types   = []\n",
    "\n",
    "    for disease in yhat_topk:\n",
    "        disease_types.append(dict_disease[disease])\n",
    "        prob_res.append(yhat[0][disease])\n",
    "    \n",
    "    print(f\"Label asli: {true_label}.\")\n",
    "    for index_disease, disease in enumerate(disease_types):\n",
    "        res = \"benar\" if true_label == disease else \"salah\"\n",
    "        print(f\"Prediksi {index_disease+1} adalah: {disease} ({res}) dengan probabilitas {prob_res[index_disease] * 100:.2f}%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
